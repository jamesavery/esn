import joblib
import numpy as np
import scipy.stats as stats
from scipy import sparse

#from esn_dev.jaxsparse import sp_dot
from esn_dev.optimize import lstsq_stable, imed_lstsq_stable, lstsq_pcr
from esn_dev.utils import _fromfile


def esncell(map_ih, hidden_size, spectral_radius=1.5, neuron_connections=10, neuron_dist='uniform'):
    """
    Create an ESN with input, and hidden weights represented as a tuple:

        esn_dev.= (Wih, Whh, bh)

    The hidden matrix (the reservoir) is a sparse matrix in turn represented
    as a tuple of values, row/column indices, and its dense shape:
        Whh = (((values, rows, cols), shape)
    Arguments:
        map_ih: An `esn_dev.input_map.InputMap`
        hidden_size: ESN hidden size
        spectral_radius: spectral radius of Whh
        neuron_connections: number of non-zero values in rows of Whh
        neuron_dist: distribution of non-zero values in Whh (uniform/normal)
    Returns:
        (Wih, Whh, bh)
    """
    nonzeros_per_row = int(neuron_connections)
    Whh = sparse_nzpr_esn_reservoir(hidden_size, spectral_radius, nonzeros_per_row,neuron_dist)

    Whh = Whh.tocoo()
    bh  = np.random.uniform(-1, 1, (hidden_size,))
    model = (map_ih,
             ((Whh.data,
              Whh.row,
              Whh.col),
             Whh.shape),
             bh)
    return model

def generate_states_scipy(esncell, xs, h0):
    """
    Apply and ESN defined by esncell (as in created from `sparse_esncell`) to
    each input in xs with the initial state h0. Each new input uses the updated
    state from the previous step.
    Arguments:
        esncell: An ESN tuple (Wih, Whh, bh)
        xs: Array of inputs. Time in first dimension.
        h0: Initial hidden state
    Returns:
        (h,hs) where
        h: Final hidden state
        hs: All hidden states
    """
    print("USING SCIPY")
    (map_ih, (Whh, shape), bh) = esncell
    Whh = sparse.coo_matrix((Whh[0], (Whh[1], Whh[2])), shape=shape,dtype=np.float64).tocsr()
    H = np.zeros([xs.shape[0],shape[0]],dtype=np.float64)
    print(f'map dtype {map_ih(xs[0]).dtype}')
    print(type(map_ih(xs[0])))
    H[0,:] = np.tanh((Whh.dot(h0))  + map_ih(xs[0]))
    print(f'H dtype {H.dtype}')

    for i in range(1,xs.shape[0]):
        H[i,:] = np.tanh((Whh.dot(H[i-1,:]))  + map_ih(xs[i]))
        
    print(f'H dtype {H.dtype}')
 
    return (h0, H)


def train(esncell, states, labels):
    """Compute the output matrix via least squares. and add it to the esncell
    tuple to create a model tuple.

    Params:
        esncell: ESN tuple (mapih, Whh, bh)
        states: state matrix as generated by 'augmented_state_matrix'
        labels: training labels (time, height, width)
    Returns:
        (mapih, Whh, bh, Who)
    """
    Who = lstsq_stable(states, labels)
    return esncell + (Who,)


def train_imed(esncell, states, inputs, labels, sigma=1.):
    """Compute the output matrix via least squares in IMED space and add it
    to the esncell tuple to cwith set_backend(pyfftw.interfaces.scipy_fft), pyfftw.interfaces.scipy_fft.set_workers(-1):
        #faster if we enable cache using pyfftw
        pyfftw.interfaces.cache.enable()
        # perform standardizing transform using frequency method of your choice
        inputs = ST_ndim_DCT(inputs,sigma=sigma,eps=eps,inverse=False)
        labels = ST_ndim_DCT(labels,sigma=sigma,eps=eps,inverse=False)
        pred_labels = ST_ndim_DCT(pred_labels,sigma=sigma,eps=eps,inverse=False)100reate a model tuple.

    Params:
        esncell: ESN tuple (mapih, Whh, bh)
        states: state matrix as generated by 'augmented_state_matrix'
        inputs: training inputs (time, height, width)
        labels: training labels (time, height, width)
    Returns:
        (mapih, Whh, bh, Who)
    """
    Who = imed_lstsq_stable(states, inputs, labels, sigma)
    return esncell + (Who,)


def predict_scipy(model, y0, h0, Npred,return_H=True):
    print('PREDICT SCIPY')
    """
    Given a trained model = (Wih,Whh,bh,Who), a start internal state h0, and input
    y0 predict in free-running mode for Npred steps into the future, with
    output feeding back y_n as next input:

      h_{n+1} = \tanh(Whh h_n + Wih y_n + bh)
      y_{n+1} = Who h_{n+1}
    """
    if y0.ndim == 1:
        aug_len = y0.shape[0] + 1
    elif y0.ndim == 2:
        aug_len = y0.shape[0] * y0.shape[1] + 1
    else:
        raise ValueError("'y0' must either be a vector or a matrix.")
        
    if return_H == False:
        print('Not returning H when predicting')
        (map_ih,(Whh,shape),bh,Who) = model
        Who = np.array(Who,dtype=np.float32)
        Whh = sparse.coo_matrix((Whh[0], (Whh[1], Whh[2])), shape=shape,dtype=np.float32).tocsr()    
        h = np.zeros([aug_len+shape[0]],dtype=np.float64)
        h[0] = 1.
        Y = np.zeros([Npred,y0.size])

        #first run
        y = y0
        h[aug_len:] = np.tanh(Whh.dot(h0[aug_len:]) + map_ih(y))
        h[1:aug_len] =y.reshape(-1)
        Y[0,:] = Who.dot(h)
        for i in range(1,Npred):
            y = Y[i-1,:]
            h[aug_len:] = np.tanh(Whh.dot(h[aug_len:]) + map_ih(y.reshape(y0.shape)) )
            h[1:aug_len] = y.reshape(-1)
            Y[i,:] = Who.dot(h)
    
        Y = Y.reshape(np.hstack((Npred,y0.shape)))

        return Y
    
def predict_reduced_sklearn(model, y0, h0, Npred,pca_object,return_H=True):
    print('PREDICT REDUCED')
    """
    Given a trained model = (Wih,Whh,bh,Who), a start internal state h0, and input
    y0 predict in free-running mode for Npred steps into the future, with
    output feeding back y_n as next input:

      h_{n+1} = \tanh(Whh h_n + Wih y_n + bh)
      y_{n+1} = Who h_{n+1}
    """
    (map_ih,(Whh,shape),bh,Who) = model
    Who = np.array(Who,dtype=np.float64)
    Whh = sparse.coo_matrix((Whh[0], (Whh[1], Whh[2])), shape=shape,dtype=np.float64).tocsr()    
    #infer number of stacked inputs and number of bias parameters in h0
    
    Nhidden = map_ih(y0).shape[0]
    max_stacks = 6
    max_Nbias  = 1
    for Nbias in range(max_Nbias+1):
        for stacks in range(max_stacks+1):
            if (h0.shape[0] == Nhidden+(y0.size)*stacks+Nbias):
                print(f'Inferred that Nbias = {Nbias}, stacks = {stacks}')
                break
        else:
            # Continue if the inner loop wasn't broken.
            continue
        # Inner loop was broken, break the outer.
        break

   
    input_size = y0.size
    if y0.ndim == 1:
        aug_len = y0.shape[0]*stacks + Nbias
    elif y0.ndim == 2:
        aug_len = (y0.shape[0] * y0.shape[1])*stacks + Nbias
    else:
        raise ValueError("'y0' must either be a vector or a matrix.")
        
    if return_H == False:
        print('Not returning H when predicting')
        h = np.array(h0)
        Y = np.zeros([Npred+stacks,y0.size])
        for s in range(stacks-1):
            Y[s] = h0[Nbias+input_size*(s+1):Nbias+input_size*(s+2)]
        Y[stacks-1] = y0.reshape(-1)
        
        for i in range(stacks,stacks+Npred):
            y = Y[i-1]

            Whh.dot(h[aug_len:])
            map_ih(y.reshape(y0.shape))
            h[aug_len:] = np.tanh(Whh.dot(h[aug_len:]) + map_ih(y.reshape(y0.shape)))
            #print(f'In Y is there nan at i={i-1}? {np.isnan(Y[i-1]).all()}')
            
            for s in range(stacks):
                h[Nbias+input_size*s:Nbias+input_size*(s+1)] = Y[i-stacks+s]
            #print(f'In h is there nan at i={i}? {np.isnan(h).all()}')
           
            #reduced h
            h_r = np.squeeze( pca_object.transform(h.reshape(1,-1)) )
           
            Y[i,:] = Who.dot(h_r)
    
        Y = Y[stacks:].reshape(np.hstack((Npred,y0.shape)))

        return Y
    
    else:
        (map_ih,(Whh,shape),bh,Who) = model
        Who = np.array(Who,dtype=np.float32)
        Whh = sparse.coo_matrix((Whh[0], (Whh[1], Whh[2])), shape=shape,dtype=np.float32).tocsr()    
        H = np.zeros([Npred,aug_len+shape[0]],dtype=np.float32)
        H[:,0] = 1.
        Y = np.zeros([Npred,y0.size],dtype=np.float32)
        #first run
        y = y0
        h = h0[aug_len:]
        h = np.tanh(Whh.dot(h) + map_ih(y))
        H[0,1:aug_len] = y.reshape(-1)
        H[0,aug_len:]  = h
        Y[0,:] = Who.dot(H[0,:])
        for i in range(1,Npred):
            (y,h) = Y[i-1,:] , H[i-1,aug_len:]
            h = np.tanh(Whh.dot(h) + map_ih(y.reshape(y0.shape)))
            H[i,1:aug_len] = y.reshape(-1)
            H[i,aug_len:]  = h
            Y[i,:] = Who.dot(H[i,:])
  
        Y = Y.reshape(np.hstack((Npred,y0.shape)))

        return ((ys[-1,:],H[-1,:]), (Y,H))
    
    
def predict_reduced(model, y0, h0, Npred,v,mean,return_H=True):
    print('PREDICT REDUCED')
    """
    Given a trained model = (Wih,Whh,bh,Who), a start internal state h0, and input
    y0 predict in free-running mode for Npred steps into the future, with
    output feeding back y_n as next input:

      h_{n+1} = \tanh(Whh h_n + Wih y_n + bh)
      y_{n+1} = Who h_{n+1}
    """
    (map_ih,(Whh,shape),bh,Who) = model
    Who = np.array(Who,dtype=np.float64)
    Whh = sparse.coo_matrix((Whh[0], (Whh[1], Whh[2])), shape=shape,dtype=np.float64).tocsr()    
    #infer number of stacked inputs and number of bias parameters in h0
    
    Nhidden = map_ih(y0).shape[0]
    max_stacks = 6
    max_Nbias  = 1
    for Nbias in range(max_Nbias+1):
        for stacks in range(max_stacks+1):
            if (h0.shape[0] == Nhidden+(y0.size)*stacks+Nbias):
                print(f'Inferred that Nbias = {Nbias}, stacks = {stacks}')
                break
        else:
            # Continue if the inner loop wasn't broken.
            continue
        # Inner loop was broken, break the outer.
        break

   
    input_size = y0.size
    if y0.ndim == 1:
        aug_len = y0.shape[0]*stacks + Nbias
    elif y0.ndim == 2:
        aug_len = (y0.shape[0] * y0.shape[1])*stacks + Nbias
    else:
        raise ValueError("'y0' must either be a vector or a matrix.")
        
    if return_H == False:
        print('Not returning H when predicting')
        h = np.array(h0)+mean
        Y = np.zeros([Npred+stacks,y0.size])
        for s in range(stacks-1):
            Y[s] = h0[Nbias+input_size*(s+1):Nbias+input_size*(s+2)]
        Y[stacks-1] = y0.reshape(-1)
        
        for i in range(stacks,stacks+Npred):
            y = Y[i-1]

            h[aug_len:] = np.tanh(Whh.dot(h[aug_len:]) + map_ih(y.reshape(y0.shape)))
            
            for s in range(stacks):
                h[Nbias+input_size*s:Nbias+input_size*(s+1)] = Y[i-stacks+s]
            #print(f'In h is there nan at i={i}? {np.isnan(h).all()}')
            
            #centered h
            h2 = h - mean
            #reduced h
            h_r = h2.dot(v)
            h_r[-1] = 1. # bias term 

            Y[i,:] = Who.dot(h_r)
    
        Y = Y[stacks:].reshape(np.hstack((Npred,y0.shape)))

        return Y
    
    else:
        (map_ih,(Whh,shape),bh,Who) = model
        Who = np.array(Who,dtype=np.float32)
        Whh = sparse.coo_matrix((Whh[0], (Whh[1], Whh[2])), shape=shape,dtype=np.float32).tocsr()    
        H = np.zeros([Npred,aug_len+shape[0]],dtype=np.float32)
        H[:,0] = 1.
        Y = np.zeros([Npred,y0.size],dtype=np.float32)
        #first run
        y = y0
        h = h0[aug_len:]
        h = np.tanh(Whh.dot(h) + map_ih(y))
        H[0,1:aug_len] = y.reshape(-1)
        H[0,aug_len:]  = h
        Y[0,:] = Who.dot(H[0,:])
        for i in range(1,Npred):
            (y,h) = Y[i-1,:] , H[i-1,aug_len:]
            h = np.tanh(Whh.dot(h) + map_ih(y.reshape(y0.shape)))
            H[i,1:aug_len] = y.reshape(-1)
            H[i,aug_len:]  = h
            Y[i,:] = Who.dot(H[i,:])
  
        Y = Y.reshape(np.hstack((Npred,y0.shape)))

        return ((ys[-1,:],H[-1,:]), (Y,H))

def predict_scipy_stack(model, y0, h0, Npred,return_H=False,v=False):
    print('PREDICT SCIPY')
    """
    Given a trained model = (Wih,Whh,bh,Who), a start internal state h0, and input
    y0 predict in free-running mode for Npred steps into the future, with
    output feeding back y_n as next input:

      h_{n+1} = \tanh(Whh h_n + Wih y_n + bh)
      y_{n+1} = Who h_{n+1}
    """
    (map_ih,(Whh,shape),bh,Who) = model
    Who = np.array(Who,dtype=np.float64)
    Whh = sparse.coo_matrix((Whh[0], (Whh[1], Whh[2])), shape=shape,dtype=np.float64).tocsr()    
    #infer number of stacked inputs and number of bias parameters in h0
    
    Nhidden = map_ih(y0).shape[0]
    max_stacks = 6
    max_Nbias  = 1
    for Nbias in range(max_Nbias+1):
        for stacks in range(max_stacks+1):
            if (h0.shape[0] == Nhidden+(y0.size)*stacks+Nbias):
                print(f'Inferred that Nbias = {Nbias}, stacks = {stacks}')
                break
        else:
            # Continue if the inner loop wasn't broken.
            continue
        # Inner loop was broken, break the outer.
        break

   
    input_size = y0.size
    if y0.ndim == 1:
        aug_len = y0.shape[0]*stacks + Nbias
    elif y0.ndim == 2:
        aug_len = (y0.shape[0] * y0.shape[1])*stacks + Nbias
    else:
        raise ValueError("'y0' must either be a vector or a matrix.")
        
    if return_H == False:
        print('Not returning H when predicting')
        h = np.array(h0)
        Y = np.zeros([Npred+stacks,y0.size])
        for s in range(stacks-1):
            Y[s] = h0[Nbias+input_size*(s+1):Nbias+input_size*(s+2)]
        Y[stacks-1] = y0.reshape(-1)
        
        for i in range(stacks,stacks+Npred):
            y = Y[i-1]

            Whh.dot(h[aug_len:])
            map_ih(y.reshape(y0.shape))
            h[aug_len:] = np.tanh(Whh.dot(h[aug_len:]) + map_ih(y.reshape(y0.shape)))
            #print(f'In Y is there nan at i={i-1}? {np.isnan(Y[i-1]).all()}')
            
            for s in range(stacks):
                h[Nbias+input_size*s:Nbias+input_size*(s+1)] = Y[i-stacks+s]
            #print(f'In h is there nan at i={i}? {np.isnan(h).all()}')
            
            Y[i,:] = Who.dot(h)
    
        Y = Y[stacks:].reshape(np.hstack((Npred,y0.shape)))

        return Y
    
    else:
        (map_ih,(Whh,shape),bh,Who) = model
        Who = np.array(Who,dtype=np.float32)
        Whh = sparse.coo_matrix((Whh[0], (Whh[1], Whh[2])), shape=shape,dtype=np.float32).tocsr()    
        H = np.zeros([Npred,aug_len+shape[0]],dtype=np.float32)
        H[:,0] = 1.
        Y = np.zeros([Npred,y0.size],dtype=np.float32)
        #first run
        y = y0
        h = h0[aug_len:]
        h = np.tanh(Whh.dot(h) + map_ih(y))
        H[0,1:aug_len] = y.reshape(-1)
        H[0,aug_len:]  = h
        Y[0,:] = Who.dot(H[0,:])
        for i in range(1,Npred):
            (y,h) = Y[i-1,:] , H[i-1,aug_len:]
            h = np.tanh(Whh.dot(h) + map_ih(y.reshape(y0.shape)))
            H[i,1:aug_len] = y.reshape(-1)
            H[i,aug_len:]  = h
            Y[i,:] = Who.dot(H[i,:])
  
        Y = Y.reshape(np.hstack((Npred,y0.shape)))

        return ((ys[-1,:],H[-1,:]), (Y,H))

def warmup_predict(model, imgs, Npred):
    """
    Given a trained ESN and a number input images 'imgs', predicts 'Npred'
    frames after the last frame of 'imgs'. The input images are used to
    create the inital state 'h0' for the prediction (warmup).
    """
    H = augmented_state_matrix(model[:-1], imgs, 0)
    h0 = H[-2]
    y0 = imgs[-1]
    return predict(model, y0, h0, Npred)


def hidden_size(esn_dev):
    (_,shape) = esn_dev[1]
    return shape[0]


def augmented_state_matrix_stack(esncell, inputs, Ntrans):
    Ntrain = inputs.shape[0]

    h0 = np.zeros(hidden_size(esncell))

    (_,H) = generate_states_scipy(esncell, inputs, h0)
    print(f'H has shape')
    
    input_shape_spatial = int(np.prod(inputs.shape[1:]))
    H0 = H[Ntrans:]

    """I0 = inputs[Ntrans:].reshape(Ntrain-Ntrans,-1)
    I1 = inputs[Ntrans-1:-1].reshape(Ntrain-Ntrans,-1)
    I2 = inputs[Ntrans-2:-2].reshape(Ntrain-Ntrans,-1)
    I3 = inputs[Ntrans-3:-3].reshape(Ntrain-Ntrans,-1)
    I4 = inputs[Ntrans-4:-4].reshape(Ntrain-Ntrans,-1)
    I5 = inputs[Ntrans-5:-5].reshape(Ntrain-Ntrans,-1)

    
    ones = np.ones((Ntrain-Ntrans,1))"""
    #H = np.array(np.concatenate([ones,I4,I3,I2,I1,I0,H0], axis=1))
    H = np.array(H0)

    """dropout_ratio = 0.2
    x_cords = np.random.randint(low=0,high=H.shape[0],size=int(H.shape[0]*dropout_ratio))
    y_cords = np.random.randint(low=0,high=H.shape[1],size=len(x_cords))
    H[x_cords,y_cords]=0"""
    return H



def sparse_nzpr_esn_reservoir(dim, spectral_radius, nonzeros_per_row,neuron_dist):
    print(f'Using {nonzeros_per_row} nonzeros per row and {neuron_dist} distribution')
    #random number generator
    """seed = np.random.get_state()[1][0]
    # Note I am using seed here
    rng = np.random.default_rng(seed)"""
    
    # values that must be sampled
    nr_values = dim * nonzeros_per_row
    
    if not (neuron_dist=='uniform' or neuron_dist=='normal'):
        print(f"neuron_dist {neuron_dist} unknown.\n"
              "Should be 'uniform' or 'normal'\n"
              "Proceeding with 'uniform'")
        neuron_dist = 'uniform'
    
    if neuron_dist == 'uniform':
        dist_gen = np.random.uniform

        #heuristic distribution param
        k = 0.4098 #pm 0.0003
       
        # Pick variance according to 
        # desired spectral radius        
        var = (spectral_radius**2   /
               (nonzeros_per_row*(1+k*dim**(-k))**2))
        
        #pick 0-symmetric interval
        high = np.sqrt(3*var)
        low  = -high
        dist_args = dict(
            high=high,
            low=low,
            size=[nr_values])
        
    elif neuron_dist == 'normal':
        dist_gen = np.random.normal

        #heuristic distribution param
        k = 0.4075 #pm 0.0003
        
        # Pick variance according to 
        # desired spectral radius
        var = (spectral_radius**2   /
               (nonzeros_per_row*(1+k*dim**(-k))**2))
        
        dist_args = dict(
            loc=0.0,
            scale=np.sqrt(var),
            size=[nr_values])
    
    rng = np.random.default_rng()
    dense_shape = (dim, dim)

    # get row_idx like: [0,0,0,1,1,1,....]
    row_idx = np.tile(np.arange(dim)[:, None], nonzeros_per_row).reshape(-1)

    # get col idx that are unique within each row
    col_idx = []
    for ii in range(dim):
        #random generate without replacement
        cols = tuple(rng.choice(dim, size=nonzeros_per_row, replace=False))
        col_idx += (cols)
    col_idx = np.asarray(col_idx)
    vals = dist_gen(**dist_args)
    
    # scipy sparse matrix
    matrix = sparse.coo_matrix((vals, (row_idx, col_idx)),shape=dense_shape)

    # matrix now has approximate spectral radius
    # if low-dim, do manual calc:
    if dim <= 5000:

        eig_max = sparse.linalg.eigs(
            matrix, 
            k=1, 
            tol=0,
            return_eigenvectors=False,
            which='LM',
            ncv=200,
            )
        
        rho = np.abs(eig_max)
        matrix = matrix.multiply(spectral_radius / rho) 
    
    return matrix


"""def device_put(model_or_cell):
    if len(model_or_cell) == 3:
        moc = _device_put_cell(model_or_cell)
    elif len(model_or_cell) == 4:
        moc = _device_put_model(model_or_cell)
    return moc


def _device_put_cell(cell):
    (mapih, (Whh,shape), bh) = cell
    mapih.device_put()
    Whh = jax.device_put(Whh)
    bh  = jax.device_put(bh)
    return (mapih, (Whh,shape), bh)


def _device_put_model(model):
    cell = model[:3]
    Who  = jax.device_put(model[3])
    return _device_put_cell(cell) + (Who,)


def load_model(filename):
    with open(filename, "rb") as fi:
        m = joblib.load(fi)
    return device_put(m)
"""